# -*- coding: utf-8 -*-
"""naivebayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rapANxqa9dAuZc-bAnQNI6PrH0rROO02
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

from google.colab import files
uploaded = files.upload()

columns = ["age", "workclass", "fnlwgt", "education", "education-num", "marital-status",
           "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss",
           "hours-per-week", "native-country", "income"]
df = pd.read_csv("adult (1).data", header=None, names=columns, index_col=False)

"""DATA ANALYSIS

"""

df.head()

df.columns

df["income"].count()

df = df.replace(' ?', pd.np.nan)

df.head()

num_null = df.isnull().sum(axis=0)
num_null

df = df.dropna(subset=['workclass', 'occupation','native-country'	])

df.shape[0]

df.head()

Y = df.income.replace({' <=50K': 0, ' >50K': 1})
Y

df.drop(columns='income',inplace=True)

X=df.values

X.shape[0]

Y=np.array(Y)

Y

Y.shape[0]

categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']
numerical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']

def traintestsplit(X,y,test_size,random_state):
  np.random.seed(random_state)
  n_test_samples = int(test_size * X.shape[0])
  permutation = np.random.permutation(X.shape[0])
  X=X[permutation]
  y = y[permutation]
  X_train = X[:-n_test_samples]
  y_train = y[:-n_test_samples]
  X_test = X[-n_test_samples:]
  y_test = y[-n_test_samples:]
  return X_train,X_test,y_train,y_test

X_train,X_test,y_train,y_test=traintestsplit(X,Y,0.34,1)

"""Naive Bayes"""

def calculate_prior_probabilities(y_train):

    class_counts = np.bincount(y_train)  # Count the number of instances of each class
    total_instances = len(y_train)  # Get the total number of instances in the training set
    prior_probabilities = class_counts / total_instances  # Calculate the prior probability of each class

    return prior_probabilities

def calculate_conditional_probabilities(X_train, y_train):

    num_features = X_train.shape[1]  # Get the number of features
    unique_classes = np.unique(y_train)  # Get the unique classes

    # Initialize dictionaries to store the conditional probabilities
    conditional_probs_positive = {}
    conditional_probs_negative = {}

    # Loop over each feature and calculate the conditional probability of each value given each class
    for feature_index in range(num_features):
        feature_values = np.unique(X_train[:, feature_index])  # Get the unique values of the current feature
        for feature_value in feature_values:
            for class_value in unique_classes:
                # Get the subset of the training set with the current feature value and class value
                subset = X_train[(X_train[:, feature_index] == feature_value) & (y_train == class_value)]

                # Calculate the conditional probability of the current feature value given the current class
                conditional_prob = len(subset) / len(X_train[y_train == class_value])

                # Add the conditional probability to the appropriate dictionary
                if class_value == 1:
                    if feature_index not in conditional_probs_positive:
                        conditional_probs_positive[feature_index] = {}
                    conditional_probs_positive[feature_index][feature_value] = conditional_prob
                else:
                    if feature_index not in conditional_probs_negative:
                        conditional_probs_negative[feature_index] = {}
                    conditional_probs_negative[feature_index][feature_value] = conditional_prob

    return conditional_probs_positive, conditional_probs_negative

"""Laplace Smoothing

"""

def calculate_conditional_probabilities_laplace(X_train, y_train, alpha=1):

    num_features = X_train.shape[1]  # Get the number of features
    unique_classes = np.unique(y_train)  # Get the unique classes
    num_classes = len(unique_classes)  # Get the number of classes

    # Initialize dictionaries to store the conditional probabilities
    conditional_probs_positive = {}
    conditional_probs_negative = {}

    # Loop over each feature and calculate the conditional probability of each value given each class
    for feature_index in range(num_features):
        feature_values = np.unique(X_train[:, feature_index])  # Get the unique values of the current feature
        for feature_value in feature_values:
            for class_value in unique_classes:
                # Get the subset of the training set with the current feature value and class value
                subset = X_train[(X_train[:, feature_index] == feature_value) & (y_train == class_value)]

                # Calculate the conditional probability of the current feature value given the current class,
                # using Laplace smoothing
                numerator = len(subset) + alpha
                denominator = len(X_train[y_train == class_value]) + alpha * len(feature_values)
                conditional_prob = numerator / denominator

                # Add the conditional probability to the appropriate dictionary
                if class_value == 1:
                    if feature_index not in conditional_probs_positive:
                        conditional_probs_positive[feature_index] = {}
                    conditional_probs_positive[feature_index][feature_value] = conditional_prob
                else:
                    if feature_index not in conditional_probs_negative:
                        conditional_probs_negative[feature_index] = {}
                    conditional_probs_negative[feature_index][feature_value] = conditional_prob

    return conditional_probs_positive, conditional_probs_negative

def predict(X, prior_probs, cond_probs_positive, cond_probs_negative):

    num_instances = X.shape[0]  # Get the number of instances in X
    num_features = X.shape[1]  # Get the number of features
    predictions = np.zeros(num_instances)  # Initialize an array to store the predicted class of each instance

    # Loop over each instance in X
    for instance_index in range(num_instances):
        positive_likelihood = prior_probs[1]  # Initialize the positive class likelihood to the prior probability
        negative_likelihood = prior_probs[0]  # Initialize the negative class likelihood to the prior probability

        # Loop over each feature in the current instance
        for feature_index in range(num_features):
            feature_value = X[instance_index, feature_index]  # Get the value of the current feature

            # If the feature value is not present in the positive class conditional probability dictionary,
            # assume a zero probability
            if feature_index not in cond_probs_positive or feature_value not in cond_probs_positive[feature_index]:
                positive_likelihood *= 0
            else:
                positive_likelihood *= cond_probs_positive[feature_index][feature_value]

            # If the feature value is not present in the negative class conditional probability dictionary,
            # assume a zero probability
            if feature_index not in cond_probs_negative or feature_value not in cond_probs_negative[feature_index]:
                negative_likelihood *= 0
            else:
                negative_likelihood *= cond_probs_negative[feature_index][feature_value]

        # Calculate the posterior probability of the positive and negative classes
        positive_posterior = positive_likelihood / (positive_likelihood + negative_likelihood)
        negative_posterior = negative_likelihood / (positive_likelihood + negative_likelihood)

        # Assign the class with the highest posterior probability as the predicted class
        if positive_posterior > negative_posterior:
            predictions[instance_index] = 1

    return predictions.astype(int)

def accuracy(predictions, y_true):

    num_correct = np.sum(predictions == y_true)  # Count the number of correct predictions
    num_instances = len(y_true)  # Get the total number of instances

    return num_correct / num_instances  # Return the accuracy as a float between 0 and 1

"""10 models without smoothing"""

def generate_tenmodels(X, y):
  Nb=[]
  for i in range(10):
    X_train,X_test,y_train,y_test=traintestsplit(X,Y,0.34,i)
    prior_probabilities=calculate_prior_probabilities(y_train)
    conditional_probs_positive,conditional_probs_negative=calculate_conditional_probabilities(X_train, y_train)
    predictions = predict(X_test, prior_probabilities, conditional_probs_positive,conditional_probs_negative)
    results = []
    acc = accuracy(predictions, y_test)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)
    results.append(acc*100)
    results.append(precision)
    results.append(recall)
    results.append(f1)
    Nb.append(results)
  return Nb

Nb = generate_tenmodels(X,Y)
Nb

"""10 models with smoothing"""

def generate_tenmodels_smoothing(X, y):
  Nb2=[]
  for i in range(10):
    X_train,X_test,y_train,y_test=traintestsplit(X,Y,0.34,i)
    prior_probabilities=calculate_prior_probabilities(y_train)
    conditional_probs_positive_smoothing,conditional_probs_negative_smoothing=calculate_conditional_probabilities_laplace(X_train, y_train)
    predictions = predict(X_test, prior_probabilities, conditional_probs_positive_smoothing,conditional_probs_negative_smoothing)
    results = []
    acc = accuracy(predictions, y_test)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)
    results.append(acc*100)
    results.append(precision)
    results.append(recall)
    results.append(f1)
    Nb2.append(results)
  return Nb2

Nb2 = generate_tenmodels_smoothing(X,Y)
Nb2

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler

encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')

def generate_tenmodels_KNN(X, y,df):
  Kn=[]
  for i in range(10):
    X_cat = encoder.fit_transform(df[categorical_features]).toarray()
    X_processed = np.concatenate((X_cat,df[numerical_features]), axis=1)
    X_train, X_test, y_train, y_test = traintestsplit(X_processed, y,0.34,i)
    knn = KNeighborsClassifier(n_neighbors=21)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    results = []
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1score = f1_score(y_test, y_pred)
    results.append(accuracy*100)
    results.append(precision)
    results.append(recall)
    results.append(f1score)
    Kn.append(results)
  return Kn

Kn=generate_tenmodels_KNN(X, Y,df)
Kn

"""Logistic Regression"""

def generate_tenmodels_LR(X, y,df):
  Lr=[]
  for i in range(10):
    X_cat = encoder.fit_transform(df[categorical_features]).toarray()
    X_processed = np.concatenate((X_cat,df[numerical_features]), axis=1)
    X_train, X_test, y_train, y_test = traintestsplit(X_processed, y,0.34,i)
    lr = LogisticRegression(random_state=42)
    lr.fit(X_train, y_train)
    y_pred = lr.predict(X_test)
    results = []
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1score = f1_score(y_test, y_pred)
    results.append(accuracy*100)
    results.append(precision)
    results.append(recall)
    results.append(f1score)
    Lr.append(results)
  return Lr

Lr=generate_tenmodels_LR(X,Y,df)
Lr

"""Comparing all 4 models

"""

def print_metrics_table(metrics_array):
    metrics_df = pd.DataFrame(metrics_array, columns=['Accuracy', 'Precision', 'Recall', 'F1-score'])
    mean_values = np.mean(metrics_array, axis=0)
    var_values = np.var(metrics_array, axis=0)
    metrics_df.loc['mean'] = mean_values
    metrics_df.loc['var'] = var_values
    print(metrics_df.to_string(index=True))

Nb=np.array(Nb)
Nb2=np.array(Nb2)
Kn=np.array(Kn)
Lr=np.array(Lr)

print_metrics_table(Nb)

print_metrics_table(Nb2)

print_metrics_table(Kn)

print_metrics_table(Lr)

